{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.documents import Document\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to read documents from different resources\n",
    "\n",
    "def read_pdf(pdf_file):\n",
    "    reader = PdfReader(pdf_file)\n",
    "    pdf_texts = [p.extract_text().strip() for p in reader.pages if p.extract_text()]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = [Document(page_content=p) for p in pdf_texts]\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    return splits\n",
    "\n",
    "def read_weblink(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    return splits\n",
    "\n",
    "def read_local_text_file(filename):\n",
    "    loader = TextLoader(filename)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OllamaEmbeddings(model=\"mixtral\")\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "# embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding and saving to vector database\n",
    "def embedding_documents(documents):\n",
    "    vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# embedding only\n",
    "def embedding_text(text):\n",
    "    return embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve documents from vector database\n",
    "\n",
    "def retrieve_documents(question):\n",
    "    chromadb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "    retriever = chromadb.as_retriever(search_type=\"similarity\")\n",
    "    docs = retriever.invoke(question)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, retrieved_documents):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant. You will be shown the user's question, and the relevant information. Respond according to the provided information\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {query}. \\n Information: {retrieved_documents}\"\n",
    "        }\n",
    "    ]\n",
    "    # print(messages)\n",
    "    llm = ChatOllama(model=\"mixtral\")\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def rag_openai(query, retrieved_documents):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant. You will be shown the user's question, and the relevant information. Respond according to the provided information\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {query}. \\n Information: {retrieved_documents}\"\n",
    "        }\n",
    "    ]\n",
    "    llm = OpenAI(openai_api_key=\"API_KEY\")\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: save document to vector database\n",
    "# docs = read_local_text_file('./docs/demo.txt')\n",
    "# docs = read_pdf('./docs/demo.pdf')\n",
    "docs = read_weblink('https://docs.scala-lang.org/overviews/scala-book/prelude-taste-of-scala.html')\n",
    "embedding_documents(docs)\n",
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: retrieve and call LLM\n",
    "search_query = 'what is the difference of var and val?'\n",
    "related_documents = retrieve_documents(search_query)\n",
    "output = rag(query=search_query, retrieved_documents=related_documents)\n",
    "print(search_query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cosine of 2 vectors\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "text = 'income'\n",
    "embedding_result1 = embedding_text(text)\n",
    "# print(embedding_result1)\n",
    "\n",
    "embedding_result2 = embedding_text('revenue')\n",
    "# print(embedding_result2)\n",
    "\n",
    "cosine = np.dot(embedding_result1, embedding_result2) / (norm(embedding_result1) * norm(embedding_result2))\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test embeddings performance\n",
    "\n",
    "test_embeddings = OllamaEmbeddings(model=\"mixtal\")\n",
    "# test_embeddings = GPT4AllEmbeddings()\n",
    "# test_embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "import time\n",
    "docs = read_local_text_file('./docs/demo.txt')\n",
    "text = docs[0].page_content\n",
    "start_time = time.time()\n",
    "query_result = test_embeddings.embed_query(text)\n",
    "print(len(query_result))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
